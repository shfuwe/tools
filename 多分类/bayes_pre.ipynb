{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/fuwen/anaconda3/envs/sw0/lib/python3.7/site-packages/jieba/__init__.py\", line 154, in initialize\n",
      "    _replace_file(fpath, cache_file)\n",
      "PermissionError: [Errno 1] Operation not permitted: '/tmp/tmpys4ospoi' -> '/tmp/jieba.cache'\n",
      "Loading model cost 0.747 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['其他', -1, ['ICT:0.07311546840958608', '新能源汽车:0.21925925925925946', '生物医药:0.05333333333333336', '医疗器械:0.021699346405228782', '钢铁:0.030588235294117676', '能源:0.09542483660130718', '工业机器人:0.2630065359477121', '先进轨道交通:0.023267973856209163', '数控机床:0.0366884531590414', '工业软件:0.013246187363834431', '高端装备:0.018649237472766887', '半导体:0.008714596949891075', '人工智能:0.012810457516339873', '稀土:0.13019607843137262']], ['其他', -1, ['ICT:0.07311546840958608', '新能源汽车:0.21925925925925946', '生物医药:0.05333333333333336', '医疗器械:0.021699346405228782', '钢铁:0.030588235294117676', '能源:0.09542483660130718', '工业机器人:0.2630065359477121', '先进轨道交通:0.023267973856209163', '数控机床:0.0366884531590414', '工业软件:0.013246187363834431', '高端装备:0.018649237472766887', '半导体:0.008714596949891075', '人工智能:0.012810457516339873', '稀土:0.13019607843137262']]]\n",
      "['其他', '其他']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import jieba\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import joblib\n",
    "\n",
    "\n",
    "# 分词\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    带有语料清洗功能的分词函数\n",
    "    \"\"\"\n",
    "    text = re.sub(\"\\{%.+?%\\}\", \" \", text)  # 去除 {%xxx%} (地理定位, 微博话题等)\n",
    "    text = re.sub(\"@.+?( |$)\", \" \", text)  # 去除 @xxx (用户名)\n",
    "    text = re.sub(\"【.+?】\", \" \", text)  # 去除 【xx】 (里面的内容通常都不是用户自己写的)\n",
    "    icons = re.findall(\"\\[.+?\\]\", text)  # 提取出所有表情图标\n",
    "    text = re.sub(\"\\[.+?\\]\", \"IconMark\", text)  # 将文本中的图标替换为`IconMark`\n",
    "\n",
    "    tokens = []\n",
    "    for k, w in enumerate(jieba.lcut(text)):\n",
    "        w = w.strip()\n",
    "        if \"IconMark\" in w:  # 将IconMark替换为原图标\n",
    "            for i in range(w.count(\"IconMark\")):\n",
    "                tokens.append(icons.pop(0))\n",
    "        elif w and w != '\\u200b' and w.isalpha():  # 只保留有效文本\n",
    "            tokens.append(w)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "pp = './data'\n",
    "# 加载模型\n",
    "model_path0 = pp+'/bayes_01.02.pkl'\n",
    "model0 = joblib.load(model_path0)\n",
    "# 加载词典\n",
    "vec_path0 = pp+'/feature_01.02.pkl'\n",
    "vec0 = CountVectorizer(decode_error=\"replace\", vocabulary=pickle.load(open(vec_path0, \"rb\")))\n",
    "\n",
    "\n",
    "# 处理数据\n",
    "def getType(string_l):\n",
    "    type_=['ICT', '新能源汽车', '生物医药', '医疗器械', '钢铁', \n",
    "       '能源', '工业机器人', '先进轨道交通', '数控机床',  '工业软件', \n",
    "       '高端装备', '半导体', '人工智能', '稀土']\n",
    "    \n",
    "    if isinstance(string_l,str):\n",
    "        string_l=[string_l]\n",
    "    if isinstance(string_l,list):\n",
    "        X_data = []\n",
    "        for string in string_l:\n",
    "            X_data.append(\" \".join(tokenize(string)))\n",
    "\n",
    "        vecc = vec0.transform(X_data)\n",
    "        result_pre = model0.predict(vecc)\n",
    "        result_pre_proba = model0.predict_proba(vecc)\n",
    "        \n",
    "        res=[]\n",
    "        for i in range(len(string_l)):\n",
    "            sin_res=[]\n",
    "            end_index=result_pre[i]\n",
    "            end_score=result_pre_proba[i][end_index]\n",
    "            if end_score<0.75:\n",
    "                sin_res.append('其他')\n",
    "                sin_res.append(-1)\n",
    "            else:\n",
    "                sin_res.append(type_[end_index])\n",
    "                sin_res.append(end_score)\n",
    "            sin_res.append([type_[j]+':'+str(result_pre_proba[i][j]) for j in range(len(type_))])\n",
    "            res.append(sin_res)\n",
    "        return res\n",
    "    else:\n",
    "        print('error')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    string = [\"妈了个逼的\",\"你是不是傻子\"]\n",
    "    result=getType(string)\n",
    "    print(result)\n",
    "    all_res=[i[0] for i in result]\n",
    "    print(all_res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sw0",
   "language": "python",
   "name": "sw0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
